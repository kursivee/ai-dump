# WORK IN PROGRESS
Paper in question: https://arxiv.org/pdf/2506.08872 Your Brain on ChatGPT: Accumulation
of Cognitive Debt when Using an AI
Assistant for Essay Writing Task
### Brain dump

Reading research papers is insanely hard because I have no background in writing any of this stuff. So I wanted AI to process this research paper and translate it to something more readable by gen pop. So first I read the abstract myself to get a better understanding of what it was about. I took some notes on what the abstract was trying to say at a high level. Then I tried uploading the PDF and asking it to translate the abstract. It gave something more like a summary cause it left out some important details. I tried asking it to translate again but this time I copy/pasted the abstract into the prompt. The copy/paste method seem to retain more detail around the study. I think it more or less did an acceptable job but something feels off to me. 

Ok I figured out what it was. So I have this idea that one key component of learning is the ability to take an idea and root it into something else. If I take the paper and translate it through chatgpt to a point to where I can understand it, sure I'll learn the gist of the paper but I won't learn about the more complicated topics that the paper brings up.

Take this excerpt from the abstract

```
We used electroencephalography (EEG) to record participants' brain 
activity in order to assess their cognitive engagement and 
cognitive load, and to gain a deeper understanding of neural
activations during the essay writing task. 
```

The chatGPT translation (more or less)
```
To see what was going on in their brains during writing,
the team used a device called EEG (electroencephalography).
This tool measures brain activity and helps understand 
how much mental effort someone is putting in—called cognitive
load—and how connected different brain regions are.
```

I think it's a good rephrase. The translation is definetly easier to digest than the original. But here's why this approach felt off to me. If I were to see the original excerpt in another paper, I would still probably be confused on what it was saying because I'm not used to it being said that way. I didn't learn how to map the more complex excerpt to something digestiable. I'd argue that I've essentially created a dependency on chatGPT to make that translation for me. I haven't read the paper yet but my guess is that this is what it's hinting towards. We're trading the process of obtaining information for information. We're trading how to obtain knowledge for knowledge. This is not something AI has to do. I think it's something we're inadvertantly letting AI do. 

 The goal shouldn't be to make complex things simple through distillation, it should be through improvement of self. Distillation is a technique for improvement, not a means to an end. 